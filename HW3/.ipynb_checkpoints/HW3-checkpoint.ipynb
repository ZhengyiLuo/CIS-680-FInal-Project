{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.image as mpimg\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.patches as patches\n",
    "import os\n",
    "import math\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "import cv2\n",
    "\n",
    "def truncated_normal_(tensor, mean=0, std=1):\n",
    "    size = tensor.shape\n",
    "    tmp = tensor.new_empty(size + (4,)).normal_()\n",
    "    valid = (tmp < 2) & (tmp > -2)\n",
    "    ind = valid.max(-1, keepdim=True)[1]\n",
    "    tensor.data.copy_(tmp.gather(-1, ind).squeeze(-1))\n",
    "    tensor.data.mul_(std).add_(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_dir = \"/home/paperspace/zen/\"\n",
    "\n",
    "cufs_imgs_dir = start_dir + \"CIS-680-Final-Project/HW3/data/cufs/imgs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cufs_Data(Dataset):\n",
    "    __xs = []\n",
    "    __ys = []\n",
    "\n",
    "    def __init__(self, folder_dataset, transform=None):\n",
    "        self.transform = transform\n",
    "        # Open and load text file including the whole training data\n",
    "        with open(folder_dataset + \"data.txt\") as f:\n",
    "            for line in f:\n",
    "                self.__xs.append(folder_dataset + line.strip())\n",
    "\n",
    "    # Override to give PyTorch access to any image on the dataset\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.__xs[index])\n",
    "        img = img.convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        # Convert image and label to torch tensors\n",
    "        img = torch.from_numpy(np.asarray(img))\n",
    "        return img, []\n",
    "\n",
    "    # Override to give PyTorch size of dataset\n",
    "    def __len__(self):\n",
    "        return len(self.__xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(cufs_imgs_dir)\n",
    "imgs = [i for i in files if i.endswith(\"png\")]\n",
    "\n",
    "np.random.seed(0)\n",
    "num_train = len(imgs)\n",
    "indices = list(range(num_train))\n",
    "split = int(num_train/5)\n",
    "\n",
    "# Random, non-contiguous split\n",
    "test_idx = np.random.choice(indices, size=split, replace=False)\n",
    "train_idx = list(set(indices) - set(test_idx))\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "test_sampler = SubsetRandomSampler(test_idx)\n",
    "\n",
    "transform_img = transforms.Compose(\n",
    "    [transforms.ToTensor()])\n",
    "dset_train = cufs_Data(cufs_imgs_dir, transform = transform_img)\n",
    "train_loader = DataLoader(dset_train, batch_size=10, shuffle=False, num_workers=1, sampler=train_sampler)\n",
    "test_loader = DataLoader(dset_train, batch_size=10, shuffle=False, num_workers=1, sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f8cf461fe48>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/cv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 399, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/paperspace/anaconda3/envs/cv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 378, in _shutdown_workers\n",
      "    self.worker_result_queue.get()\n",
      "  File \"/home/paperspace/anaconda3/envs/cv/lib/python3.6/multiprocessing/queues.py\", line 337, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/paperspace/anaconda3/envs/cv/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 151, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/home/paperspace/anaconda3/envs/cv/lib/python3.6/multiprocessing/resource_sharer.py\", line 58, in detach\n",
      "    return reduction.recv_handle(conn)\n",
      "  File \"/home/paperspace/anaconda3/envs/cv/lib/python3.6/multiprocessing/reduction.py\", line 182, in recv_handle\n",
      "    return recvfds(s, 1)[0]\n",
      "  File \"/home/paperspace/anaconda3/envs/cv/lib/python3.6/multiprocessing/reduction.py\", line 155, in recvfds\n",
      "    raise EOFError\n",
      "EOFError: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8cb2eab4e0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "inputs, _ = dataiter.next()\n",
    "img = np.transpose(inputs[0].numpy(), (1,2,0))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/188 (0%)]\tLoss: 8613.848438\n",
      "====> Epoch: 1 Average loss: 5741.0070\n",
      "====> Test set loss: 1073.7242\n",
      "Train Epoch: 2 [0/188 (0%)]\tLoss: 5232.779688\n",
      "====> Epoch: 2 Average loss: 4180.2595\n",
      "====> Test set loss: 987.7456\n",
      "Train Epoch: 3 [0/188 (0%)]\tLoss: 5073.096875\n",
      "====> Epoch: 3 Average loss: 3757.7526\n",
      "====> Test set loss: 872.0054\n",
      "Train Epoch: 4 [0/188 (0%)]\tLoss: 4209.042969\n",
      "====> Epoch: 4 Average loss: 3459.3665\n",
      "====> Test set loss: 850.2905\n",
      "Train Epoch: 5 [0/188 (0%)]\tLoss: 4335.988672\n",
      "====> Epoch: 5 Average loss: 3294.6319\n",
      "====> Test set loss: 807.9850\n",
      "Train Epoch: 6 [0/188 (0%)]\tLoss: 4032.567578\n",
      "====> Epoch: 6 Average loss: 3120.6577\n",
      "====> Test set loss: 822.0522\n",
      "Train Epoch: 7 [0/188 (0%)]\tLoss: 3679.176562\n",
      "====> Epoch: 7 Average loss: 3000.2609\n",
      "====> Test set loss: 803.6178\n",
      "Train Epoch: 8 [0/188 (0%)]\tLoss: 3483.657813\n",
      "====> Epoch: 8 Average loss: 2888.7748\n",
      "====> Test set loss: 791.4803\n",
      "Train Epoch: 9 [0/188 (0%)]\tLoss: 3343.694922\n",
      "====> Epoch: 9 Average loss: 2746.3126\n",
      "====> Test set loss: 787.4258\n",
      "Train Epoch: 10 [0/188 (0%)]\tLoss: 2962.336719\n",
      "====> Epoch: 10 Average loss: 2618.9964\n",
      "====> Test set loss: 772.5230\n",
      "Train Epoch: 11 [0/188 (0%)]\tLoss: 2695.390234\n",
      "====> Epoch: 11 Average loss: 2481.3464\n",
      "====> Test set loss: 807.4129\n",
      "Train Epoch: 12 [0/188 (0%)]\tLoss: 2996.957227\n",
      "====> Epoch: 12 Average loss: 2370.7816\n",
      "====> Test set loss: 798.5546\n",
      "Train Epoch: 13 [0/188 (0%)]\tLoss: 2485.548633\n",
      "====> Epoch: 13 Average loss: 2186.0403\n",
      "====> Test set loss: 801.0025\n",
      "Train Epoch: 14 [0/188 (0%)]\tLoss: 2494.996680\n",
      "====> Epoch: 14 Average loss: 2150.1708\n",
      "====> Test set loss: 855.5482\n",
      "Train Epoch: 15 [0/188 (0%)]\tLoss: 2519.356641\n",
      "====> Epoch: 15 Average loss: 2108.1813\n",
      "====> Test set loss: 822.3400\n",
      "Train Epoch: 16 [0/188 (0%)]\tLoss: 2331.313672\n",
      "====> Epoch: 16 Average loss: 1964.5260\n",
      "====> Test set loss: 850.8478\n",
      "Train Epoch: 17 [0/188 (0%)]\tLoss: 2329.676562\n",
      "====> Epoch: 17 Average loss: 1889.0536\n",
      "====> Test set loss: 864.1746\n",
      "Train Epoch: 18 [0/188 (0%)]\tLoss: 2375.655469\n",
      "====> Epoch: 18 Average loss: 1762.8618\n",
      "====> Test set loss: 899.5100\n",
      "Train Epoch: 19 [0/188 (0%)]\tLoss: 2068.997656\n",
      "====> Epoch: 19 Average loss: 1655.7408\n",
      "====> Test set loss: 937.5002\n",
      "Train Epoch: 20 [0/188 (0%)]\tLoss: 2161.425391\n",
      "====> Epoch: 20 Average loss: 1581.2798\n",
      "====> Test set loss: 905.5719\n",
      "Train Epoch: 21 [0/188 (0%)]\tLoss: 1649.603711\n",
      "====> Epoch: 21 Average loss: 1404.9706\n",
      "====> Test set loss: 986.9612\n",
      "Train Epoch: 22 [0/188 (0%)]\tLoss: 1494.714258\n",
      "====> Epoch: 22 Average loss: 1280.0848\n",
      "====> Test set loss: 982.5668\n",
      "Train Epoch: 23 [0/188 (0%)]\tLoss: 1359.322363\n",
      "====> Epoch: 23 Average loss: 1250.4522\n",
      "====> Test set loss: 985.4758\n",
      "Train Epoch: 24 [0/188 (0%)]\tLoss: 1280.669043\n",
      "====> Epoch: 24 Average loss: 1133.9203\n",
      "====> Test set loss: 1144.5332\n",
      "Train Epoch: 25 [0/188 (0%)]\tLoss: 1357.753613\n",
      "====> Epoch: 25 Average loss: 1025.8295\n",
      "====> Test set loss: 1059.4732\n",
      "Train Epoch: 26 [0/188 (0%)]\tLoss: 1128.717773\n",
      "====> Epoch: 26 Average loss: 929.5569\n",
      "====> Test set loss: 1105.6435\n",
      "Train Epoch: 27 [0/188 (0%)]\tLoss: 956.761328\n",
      "====> Epoch: 27 Average loss: 884.2762\n",
      "====> Test set loss: 1147.3768\n",
      "Train Epoch: 28 [0/188 (0%)]\tLoss: 1171.102344\n",
      "====> Epoch: 28 Average loss: 791.5121\n",
      "====> Test set loss: 1154.5384\n",
      "Train Epoch: 29 [0/188 (0%)]\tLoss: 875.536328\n",
      "====> Epoch: 29 Average loss: 702.3689\n",
      "====> Test set loss: 1161.9227\n",
      "Train Epoch: 30 [0/188 (0%)]\tLoss: 740.851367\n",
      "====> Epoch: 30 Average loss: 635.5522\n",
      "====> Test set loss: 1198.0075\n",
      "Train Epoch: 31 [0/188 (0%)]\tLoss: 715.062158\n",
      "====> Epoch: 31 Average loss: 584.0258\n",
      "====> Test set loss: 1261.7835\n",
      "Train Epoch: 32 [0/188 (0%)]\tLoss: 727.032129\n",
      "====> Epoch: 32 Average loss: 564.2773\n",
      "====> Test set loss: 1250.0714\n",
      "Train Epoch: 33 [0/188 (0%)]\tLoss: 666.107520\n",
      "====> Epoch: 33 Average loss: 541.7530\n",
      "====> Test set loss: 1297.8278\n",
      "Train Epoch: 34 [0/188 (0%)]\tLoss: 630.079980\n",
      "====> Epoch: 34 Average loss: 490.0190\n",
      "====> Test set loss: 1347.1797\n",
      "Train Epoch: 35 [0/188 (0%)]\tLoss: 586.541162\n",
      "====> Epoch: 35 Average loss: 448.1371\n",
      "====> Test set loss: 1305.9308\n",
      "Train Epoch: 36 [0/188 (0%)]\tLoss: 505.162012\n",
      "====> Epoch: 36 Average loss: 427.8053\n",
      "====> Test set loss: 1342.9142\n",
      "Train Epoch: 37 [0/188 (0%)]\tLoss: 501.812598\n",
      "====> Epoch: 37 Average loss: 421.9072\n",
      "====> Test set loss: 1339.8294\n",
      "Train Epoch: 38 [0/188 (0%)]\tLoss: 433.100391\n",
      "====> Epoch: 38 Average loss: 378.8886\n",
      "====> Test set loss: 1388.6710\n",
      "Train Epoch: 39 [0/188 (0%)]\tLoss: 431.060693\n",
      "====> Epoch: 39 Average loss: 357.5992\n",
      "====> Test set loss: 1326.8879\n",
      "Train Epoch: 40 [0/188 (0%)]\tLoss: 387.511353\n",
      "====> Epoch: 40 Average loss: 342.4548\n",
      "====> Test set loss: 1363.6624\n",
      "Train Epoch: 41 [0/188 (0%)]\tLoss: 394.820703\n",
      "====> Epoch: 41 Average loss: 323.2665\n",
      "====> Test set loss: 1426.1612\n",
      "Train Epoch: 42 [0/188 (0%)]\tLoss: 389.908472\n",
      "====> Epoch: 42 Average loss: 313.6958\n",
      "====> Test set loss: 1342.4012\n",
      "Train Epoch: 43 [0/188 (0%)]\tLoss: 367.210498\n",
      "====> Epoch: 43 Average loss: 301.3649\n",
      "====> Test set loss: 1400.8258\n",
      "Train Epoch: 44 [0/188 (0%)]\tLoss: 353.566870\n",
      "====> Epoch: 44 Average loss: 294.3813\n",
      "====> Test set loss: 1382.8126\n",
      "Train Epoch: 45 [0/188 (0%)]\tLoss: 349.214355\n",
      "====> Epoch: 45 Average loss: 286.8077\n",
      "====> Test set loss: 1442.7290\n",
      "Train Epoch: 46 [0/188 (0%)]\tLoss: 357.941870\n",
      "====> Epoch: 46 Average loss: 288.3091\n",
      "====> Test set loss: 1436.0132\n",
      "Train Epoch: 47 [0/188 (0%)]\tLoss: 384.784106\n",
      "====> Epoch: 47 Average loss: 285.3301\n",
      "====> Test set loss: 1420.7865\n",
      "Train Epoch: 48 [0/188 (0%)]\tLoss: 343.331079\n",
      "====> Epoch: 48 Average loss: 273.7968\n",
      "====> Test set loss: 1461.6553\n",
      "Train Epoch: 49 [0/188 (0%)]\tLoss: 300.234985\n",
      "====> Epoch: 49 Average loss: 268.4575\n",
      "====> Test set loss: 1403.2184\n",
      "Train Epoch: 50 [0/188 (0%)]\tLoss: 313.718335\n",
      "====> Epoch: 50 Average loss: 271.5783\n",
      "====> Test set loss: 1539.4402\n",
      "Train Epoch: 51 [0/188 (0%)]\tLoss: 338.101221\n",
      "====> Epoch: 51 Average loss: 274.1041\n",
      "====> Test set loss: 1444.7633\n",
      "Train Epoch: 52 [0/188 (0%)]\tLoss: 411.694678\n",
      "====> Epoch: 52 Average loss: 296.6487\n",
      "====> Test set loss: 1494.5649\n",
      "Train Epoch: 53 [0/188 (0%)]\tLoss: 392.144385\n",
      "====> Epoch: 53 Average loss: 329.4512\n",
      "====> Test set loss: 1547.9619\n",
      "Train Epoch: 54 [0/188 (0%)]\tLoss: 365.515234\n",
      "====> Epoch: 54 Average loss: 351.0915\n",
      "====> Test set loss: 1476.8517\n",
      "Train Epoch: 55 [0/188 (0%)]\tLoss: 394.200537\n",
      "====> Epoch: 55 Average loss: 359.5039\n",
      "====> Test set loss: 1547.8547\n",
      "Train Epoch: 56 [0/188 (0%)]\tLoss: 550.649170\n",
      "====> Epoch: 56 Average loss: 396.2703\n",
      "====> Test set loss: 1545.7227\n",
      "Train Epoch: 57 [0/188 (0%)]\tLoss: 544.863574\n",
      "====> Epoch: 57 Average loss: 390.4663\n",
      "====> Test set loss: 1622.5400\n",
      "Train Epoch: 58 [0/188 (0%)]\tLoss: 615.981201\n",
      "====> Epoch: 58 Average loss: 355.6268\n",
      "====> Test set loss: 1548.5425\n",
      "Train Epoch: 59 [0/188 (0%)]\tLoss: 346.914111\n",
      "====> Epoch: 59 Average loss: 300.5544\n",
      "====> Test set loss: 1500.5898\n",
      "Train Epoch: 60 [0/188 (0%)]\tLoss: 342.133252\n",
      "====> Epoch: 60 Average loss: 265.4115\n",
      "====> Test set loss: 1507.6835\n",
      "Train Epoch: 61 [0/188 (0%)]\tLoss: 331.086841\n",
      "====> Epoch: 61 Average loss: 261.8793\n",
      "====> Test set loss: 1589.8481\n",
      "Train Epoch: 62 [0/188 (0%)]\tLoss: 305.606348\n",
      "====> Epoch: 62 Average loss: 239.1393\n",
      "====> Test set loss: 1588.1676\n",
      "Train Epoch: 63 [0/188 (0%)]\tLoss: 283.211450\n",
      "====> Epoch: 63 Average loss: 223.1539\n",
      "====> Test set loss: 1606.2930\n",
      "Train Epoch: 64 [0/188 (0%)]\tLoss: 267.711914\n",
      "====> Epoch: 64 Average loss: 225.1612\n",
      "====> Test set loss: 1511.6728\n",
      "Train Epoch: 65 [0/188 (0%)]\tLoss: 293.194312\n",
      "====> Epoch: 65 Average loss: 243.4753\n",
      "====> Test set loss: 1559.8557\n",
      "Train Epoch: 66 [0/188 (0%)]\tLoss: 303.545508\n",
      "====> Epoch: 66 Average loss: 277.8570\n",
      "====> Test set loss: 1619.1066\n",
      "Train Epoch: 67 [0/188 (0%)]\tLoss: 384.217212\n",
      "====> Epoch: 67 Average loss: 371.0546\n",
      "====> Test set loss: 1706.6752\n",
      "Train Epoch: 68 [0/188 (0%)]\tLoss: 523.745605\n",
      "====> Epoch: 68 Average loss: 522.4122\n",
      "====> Test set loss: 1893.7930\n",
      "Train Epoch: 69 [0/188 (0%)]\tLoss: 1935.985156\n",
      "====> Epoch: 69 Average loss: 919.3550\n",
      "====> Test set loss: 1588.5830\n",
      "Train Epoch: 70 [0/188 (0%)]\tLoss: 1620.055957\n",
      "====> Epoch: 70 Average loss: 1381.7473\n",
      "====> Test set loss: 1535.6963\n",
      "Train Epoch: 71 [0/188 (0%)]\tLoss: 2635.839844\n",
      "====> Epoch: 71 Average loss: 1762.5164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set loss: 1474.1329\n",
      "Train Epoch: 72 [0/188 (0%)]\tLoss: 2069.851367\n",
      "====> Epoch: 72 Average loss: 1605.3813\n",
      "====> Test set loss: 1408.6956\n",
      "Train Epoch: 73 [0/188 (0%)]\tLoss: 1721.202344\n",
      "====> Epoch: 73 Average loss: 1320.7569\n",
      "====> Test set loss: 1649.5007\n",
      "Train Epoch: 74 [0/188 (0%)]\tLoss: 1416.256934\n",
      "====> Epoch: 74 Average loss: 1027.8599\n",
      "====> Test set loss: 1766.0049\n",
      "Train Epoch: 75 [0/188 (0%)]\tLoss: 1295.811328\n",
      "====> Epoch: 75 Average loss: 767.0739\n",
      "====> Test set loss: 1523.5969\n",
      "Train Epoch: 76 [0/188 (0%)]\tLoss: 587.471582\n",
      "====> Epoch: 76 Average loss: 548.1628\n",
      "====> Test set loss: 1488.2044\n",
      "Train Epoch: 77 [0/188 (0%)]\tLoss: 565.103906\n",
      "====> Epoch: 77 Average loss: 380.8459\n",
      "====> Test set loss: 1540.3467\n",
      "Train Epoch: 78 [0/188 (0%)]\tLoss: 427.081787\n",
      "====> Epoch: 78 Average loss: 297.8801\n",
      "====> Test set loss: 1671.0187\n",
      "Train Epoch: 79 [0/188 (0%)]\tLoss: 303.020386\n",
      "====> Epoch: 79 Average loss: 259.6896\n",
      "====> Test set loss: 1651.3196\n",
      "Train Epoch: 80 [0/188 (0%)]\tLoss: 294.575879\n",
      "====> Epoch: 80 Average loss: 234.4764\n",
      "====> Test set loss: 1680.2365\n",
      "Train Epoch: 81 [0/188 (0%)]\tLoss: 278.598560\n",
      "====> Epoch: 81 Average loss: 225.6866\n",
      "====> Test set loss: 1771.5287\n",
      "Train Epoch: 82 [0/188 (0%)]\tLoss: 286.000049\n",
      "====> Epoch: 82 Average loss: 215.1755\n",
      "====> Test set loss: 1612.8726\n",
      "Train Epoch: 83 [0/188 (0%)]\tLoss: 250.404321\n",
      "====> Epoch: 83 Average loss: 208.7618\n",
      "====> Test set loss: 1631.5098\n",
      "Train Epoch: 84 [0/188 (0%)]\tLoss: 241.248975\n",
      "====> Epoch: 84 Average loss: 195.8014\n",
      "====> Test set loss: 1666.4664\n",
      "Train Epoch: 85 [0/188 (0%)]\tLoss: 224.160352\n",
      "====> Epoch: 85 Average loss: 191.2226\n",
      "====> Test set loss: 1673.1518\n",
      "Train Epoch: 86 [0/188 (0%)]\tLoss: 233.228760\n",
      "====> Epoch: 86 Average loss: 191.5872\n",
      "====> Test set loss: 1660.6444\n",
      "Train Epoch: 87 [0/188 (0%)]\tLoss: 244.588818\n",
      "====> Epoch: 87 Average loss: 185.7252\n",
      "====> Test set loss: 1628.3303\n",
      "Train Epoch: 88 [0/188 (0%)]\tLoss: 226.278979\n",
      "====> Epoch: 88 Average loss: 187.1999\n",
      "====> Test set loss: 1734.7076\n",
      "Train Epoch: 89 [0/188 (0%)]\tLoss: 205.132104\n",
      "====> Epoch: 89 Average loss: 178.2924\n",
      "====> Test set loss: 1723.5221\n",
      "Train Epoch: 90 [0/188 (0%)]\tLoss: 241.627490\n",
      "====> Epoch: 90 Average loss: 177.5135\n",
      "====> Test set loss: 1727.2851\n",
      "Train Epoch: 91 [0/188 (0%)]\tLoss: 221.454712\n",
      "====> Epoch: 91 Average loss: 171.9127\n",
      "====> Test set loss: 1761.9548\n",
      "Train Epoch: 92 [0/188 (0%)]\tLoss: 203.285107\n",
      "====> Epoch: 92 Average loss: 165.8317\n",
      "====> Test set loss: 1600.1976\n",
      "Train Epoch: 93 [0/188 (0%)]\tLoss: 228.495728\n",
      "====> Epoch: 93 Average loss: 168.9349\n",
      "====> Test set loss: 1751.8468\n",
      "Train Epoch: 94 [0/188 (0%)]\tLoss: 214.224634\n",
      "====> Epoch: 94 Average loss: 169.9691\n",
      "====> Test set loss: 1838.5218\n",
      "Train Epoch: 95 [0/188 (0%)]\tLoss: 210.848047\n",
      "====> Epoch: 95 Average loss: 176.7836\n",
      "====> Test set loss: 1700.4242\n",
      "Train Epoch: 96 [0/188 (0%)]\tLoss: 289.831641\n",
      "====> Epoch: 96 Average loss: 187.2264\n",
      "====> Test set loss: 1805.9154\n",
      "Train Epoch: 97 [0/188 (0%)]\tLoss: 249.122559\n",
      "====> Epoch: 97 Average loss: 181.6592\n",
      "====> Test set loss: 1733.9055\n",
      "Train Epoch: 98 [0/188 (0%)]\tLoss: 195.554956\n",
      "====> Epoch: 98 Average loss: 164.2868\n",
      "====> Test set loss: 1645.0704\n",
      "Train Epoch: 99 [0/188 (0%)]\tLoss: 209.818726\n",
      "====> Epoch: 99 Average loss: 164.1401\n",
      "====> Test set loss: 1670.3598\n",
      "Train Epoch: 100 [0/188 (0%)]\tLoss: 215.549536\n",
      "====> Epoch: 100 Average loss: 162.5849\n",
      "====> Test set loss: 1721.0759\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 10\n",
    "log_interval = 1000\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#     datasets.MNIST('../data', train=True, download=True,\n",
    "#                    transform=transforms.ToTensor()),\n",
    "#     batch_size=batch_size, shuffle=True)\n",
    "# test_loader = torch.utils.data.DataLoader(\n",
    "#     datasets.MNIST('../data', train=False, transform=transforms.ToTensor()),\n",
    "#     batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(12288, 4000)\n",
    "        self.fc21 = nn.Linear(4000, 200)\n",
    "        self.fc22 = nn.Linear(4000, 200)\n",
    "        self.fc3 = nn.Linear(200, 4000)\n",
    "        self.fc4 = nn.Linear(4000, 12288)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 12288))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "\n",
    "model = VAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 12288), reduction='sum')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      recon_batch.view(batch_size, 3, 64, 64)[:n]])\n",
    "                save_image(comparison.cpu(),\n",
    "                         'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    with torch.no_grad():\n",
    "        sample = torch.randn(64, 200).to(device)\n",
    "        sample = model.decode(sample).cpu()\n",
    "        save_image(sample.view(64, 3, 64, 64),\n",
    "                   'results/sample_' + str(epoch) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cv]",
   "language": "python",
   "name": "conda-env-cv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
